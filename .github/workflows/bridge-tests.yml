name: Bridge Loop Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'dreamos/core/bridge/**'
      - 'tests/core/bridge/**'
      - 'scripts/test_bridge_loop.py'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'dreamos/core/bridge/**'
      - 'tests/core/bridge/**'
      - 'scripts/test_bridge_loop.py'
  workflow_dispatch:  # Manual trigger

jobs:
  bridge-tests:
    name: Bridge Loop Test Suite
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: [3.9, 3.10]
        iterations: [1, 3]  # Run 1x and 3x for stability check
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements-test.txt
        pip install pytest pytest-asyncio pytest-cov
    
    - name: Run bridge tests
      run: |
        python scripts/test_bridge_loop.py -i ${{ matrix.iterations }}
      env:
        DREAMOS_TEST_MODE: "1"
        PYTHONPATH: ${{ github.workspace }}
    
    - name: Upload test metrics
      uses: actions/upload-artifact@v3
      if: always()  # Upload even if tests fail
      with:
        name: bridge-test-metrics-${{ matrix.python-version }}-${{ matrix.iterations }}
        path: test_metrics/
        retention-days: 30
    
    - name: Check for test flakes
      if: matrix.iterations > 1
      run: |
        # Install jq for JSON processing
        sudo apt-get install -y jq
        
        # Find latest metrics file
        LATEST_METRICS=$(ls -t test_metrics/bridge_tests_*.json | head -n1)
        
        # Check if any test failed in any iteration
        FAILED_TESTS=$(jq '[.[] | select(.failed_tests > 0)] | length' "$LATEST_METRICS")
        if [ "$FAILED_TESTS" -gt 0 ]; then
          echo "❌ Found flaky tests in $LATEST_METRICS"
          jq -r '.[] | select(.failed_tests > 0) | .errors[] | "  - \(.test): \(.error)"' "$LATEST_METRICS"
          exit 1
        fi
        
        # Check for timing anomalies
        jq -r '.[] | .test_times | to_entries[] | "\(.key): \(.value)"' "$LATEST_METRICS" | while read -r test time; do
          if (( $(echo "$time > 5.0" | bc -l) )); then
            echo "⚠️ Slow test detected: $test took ${time}s"
          fi
        done
    
    - name: Generate test report
      if: always()
      run: |
        # Create summary report
        echo "# Bridge Test Report" > test-report.md
        echo "Generated: $(date)" >> test-report.md
        echo "" >> test-report.md
        
        # Add Python version info
        echo "## Environment" >> test-report.md
        echo "- Python: ${{ matrix.python-version }}" >> test-report.md
        echo "- Iterations: ${{ matrix.iterations }}" >> test-report.md
        echo "" >> test-report.md
        
        # Add test results
        echo "## Test Results" >> test-report.md
        LATEST_METRICS=$(ls -t test_metrics/bridge_tests_*.json | head -n1)
        if [ -f "$LATEST_METRICS" ]; then
          jq -r '.[] | "### Iteration \(.start_time)\n- Duration: \(.end_time - .start_time)s\n- Total: \(.total_tests)\n- Passed: \(.passed_tests)\n- Failed: \(.failed_tests)\n- Skipped: \(.skipped_tests)\n"' "$LATEST_METRICS" >> test-report.md
        fi
        
        # Add timing statistics
        echo "## Test Timing" >> test-report.md
        if [ -f "$LATEST_METRICS" ]; then
          jq -r '.[] | .test_times | to_entries[] | "- \(.key): \(.value)s"' "$LATEST_METRICS" | sort -k2 -nr >> test-report.md
        fi
    
    - name: Upload test report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: bridge-test-report-${{ matrix.python-version }}-${{ matrix.iterations }}
        path: test-report.md
        retention-days: 30
    
    - name: Notify on failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('test-report.md', 'utf8');
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## Bridge Tests Failed ❌\n\n${report}`
          }); 